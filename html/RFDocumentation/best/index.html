<!DOCTYPE html>
<html>
  <head>
    <title>Robot Framework Best Practices</title>
    <link href = "./../../common.css" rel = "stylesheet">
    <style>
      td {text-align: center; border: black 1px solid;}
      th {text-align: center; border: black 1px solid;}
    </style>
    <script>
    </script>
  </head>
  <body>
    <h2>Robot Framework Best Practices</h2>

    <h3>Table of Contents</h3>
    <ul>
      <li><a href="#ts">Test Setup</a></li>
        <ul>
          <li><a href="#re">Required Elements</a></li>
          <li><a href="#popc">Permanence of Python Components</a></li>
          <li><a href="#prf">Prefer Resource Files</a></li>
        </ul>
      <li><a href="#bnt">Building New Tests</a></li>
        <ul>
          <li><a href="#ttm">The Three Monsters</a></li>
          <li><a href="#etr">Export to Resource</a></li>
          <li><a href="#cs">Combining Scripts</a></li>
          <li><a href="#cof">Conservation of Familiarity</a></li>
        </ul>
      <li><a href="#ddt">Data Driven Testing</a></li>
        <ul>
          <li><a href="#istc">Identifying Similar Test Cases</a></li>
          <li><a href="#fast">Forging a Single Test</a></li>
          <li><a href="#ttct">The Test Cases Table</a></li>
          <li><a href="#aftt">Arguments for Test Template</a></li>
          <li><a href="#wttt">Writing the Test Template</a></li>
          <li><a href="#bod">Benefits of DDT</a></li>
        </ul>
      <li><a href="#fto">Formatting The OQE</a></li>
        <ul>
          <li><a href="#romp">Rows on Main Page</a></li>
          <li><a href="#bomp">Blocks on Main Page</a></li>
          <li><a href="#ar">Assert Row</a></li>
          <li><a href="#dr">Data Row</a></li>
          <li><a href="#pr">Picture Row</a></li>
          <li><a href="#kt">Keyword Teardown</a></li>
          <li><a href="#db">Functional Groups</a></li>
        </ul>
      <li><a href="#oaa">OQE and Analysis</a></li>
        <ul>
          <li><a href="#afr">Assertions for Requirements</a></li>
          <li><a href="#mafr">Multiple Assertions for Requirements</a></li>
        </ul>
    </ul>

<a name="ts"><h3>Test Setup</h3></a>
<a name="re"><h4>Required Elements</h4></a>
<p>
Every Robot Framework script has required headers that define key portions of the script. As shown in the table below, these are (mostly)
common for .robot and .resource files. Note that Test Cases do not exist in .resource files. Note also that resource files inherit from 
each other and .robot files can inherit from .resource whiles. That means that if you define a setting, variable, or keyword in some 
.resource file and then import that resource file in another script, you automatically get everything that was previously defined.
</p>

<table>
  <tr><th>Header</th><th>Robot</th><th>Resource</th></tr>
  <tr><td><code>*** Settings ***</code></td><td class="green">&#10004;</td><td class="green">&#10004;</td></tr>
  <tr><td><code>*** Variables ***</code></td><td class="green">&#10004;</td><td class="green">&#10004;</td></tr>
  <tr><td><code>*** Keywords ***</code></td><td class="green">&#10004;</td><td class="green">&#10004;</td></tr>
  <tr><td><code>*** Test Cases ***</code></td><td class="green">&#10004;</td><td class="red">&#10008;</td></tr>
</table>

<a name="popc"><h4>Permanence of Python Components</h4></a>
<p>
By default, all python scripts are supposed to be stateless with functions/keywords that are defined for a single 
operation. Stateless means that it doesn't hold any information whether you did the task one time or a hundred.
Python also allows you to create objects and these objects can have state. Even so, the Robot Framework script 
takes each test case separately, including any object that is created during the test. If an object is initialized
and used in test case #1, then it is destroyed and must be recreated for test case #2.

<pre><code>*** Test Cases ***
Do Something
   Take Action and indidentally create object X    Alpha  <i>#creates the object X with the name 'Alpha'</i>
   x.Print  <i>#this would print 'Alpha'</i>
<i># here the object x is destroyed and the memory of 'Alpha' is lost</i>

Do Something Else
   <i>#x.Print  #this would fail and create an error because there is no object x</i>
   Take Action and indidentally create object X    Beta  <i>#creates a new object X with the name 'Beta'</i>
   x.Print  <i>#this would print 'Beta'</i>
</code></pre>
</p>

<p>
That is the default behavior, but you may also want to keep some object for several test cases. To do that,
add one line to the Python file which will alter the scope of the object in RF. This is one of the options
and it makes the object "live" throughout the whole of one test script. Additional details, including links
to the relevant RF documentation, can be found <a href="https://stackoverflow.com/questions/70285982/in-robot-framework-can-you-create-one-resource-object-as-with-libraries">here</a><br><br>

<code>ROBOT_LIBRARY_SCOPE = 'SUITE'</code>
</p>

<a name="prf"><h4>Prefer Resource Files</h4></a>
<p>
Robot test scripts and the keywords developed in those scripts are one-and-done. They exist in that script
but they are not accessible elsewhere. For reasons that do not require full explanation here, simply copying
and pasting keywords between files is NOT the preferred means of replicating keywords. Also prefer a resource
file as the location for anything which might be reused. Keywords created for one script should be reviewed
and exported to the resource file as soon as the test script is functional. Once a test script is complete,
go through and justify why any keyword exists in that test script rather than a resource file. If there is
no or little justification, it needs to move.
</p>

<a name="bnt"><h3>Building New Tests</h3></a>
<a name="ttm"><h4>The Three Monsters</h4></a>
<p>
There is a story used in refactoring circles to describe how one deals with a <em>lack</em> of knowledge
when developing a script. The first time you are faced with a task you treat it as a monster that you just
need to slay this once. In many cases you will be right. When you are faced with the second monster, you grit
your teeth and do it again, hoping that this time is the last. If you ever come upon the third monster, now
is the time to refactor.
</p>

<p>
As with writing code, test scripts often have this kind of similarity. The naive approach is to write a new
.robot file for every test and then each test can be run independently in all its glory. Unfortunately, this
approach leads to a lot of redundancy, and version skew over time. Consider that you might build the first
test early on and before you have fully developed the OQE reporting requirements. When the second test is
built, you now have all the OQE requirements and you add them <em>without</em> altering the first test. By
the time the third test is built you are also exportin all your logs to a central location and you include
that in the third test script without updating the first two.
</p>

<a name="etr"><h4>Export to Resource</h4></a>
<p>
These problems can partially be solved by exporting most keywords to a resource file. If the function already
exists in a resource file and you add OQE reporting to it, that OQE will now be reported for <em>both</em>
tests. If you add logging, that too will be in all three scripts. Most importantly, the test script themselves
will look very similar and prepared for DDT. This will help you as you try to combine them.
</p>

<a name="cs"><h4>Combining Scripts</h4></a>
<p>
In order to compare and contrast any three scripts, it is easiest line them up side by side. As you do,
look for things that are missing from or found only in one script of the three. Adjust the spacing between the
lines of the other scripts to compensate for the extra or missing pieces. If you have exported all your keywords
to a .resource file, the only differences may be parameters (a most ideal state), but you may have to ask questions
about any anomalies. If something is only found in one place, was that a unique requirement OR should it have
appeared in all? Maybe it should be replicated across all test cases. In handling these questions, it is
helpful to have expert testers on hand. Even so, the answer may be "just automate the test as it is". That 
often occurs when any test procedure is the outgrowth of many hundreds of man hours and the specific knowledge
may no longer exist.
</p>

<p>
Once the scripts have been properly compared, it should be possible to build a single script that allows for all
variations. This approach is called Data Driven Testing or DDT. DDT is the efficient way to build new tests or
add functionality to old tests. While building new tests, watch for the three monsters and then combine them into
a single DDT test as often as possible.
</p>

<a name="cof"><h4>Conservation of Familiarity</h4></a>
<p>
According to <a href="https://en.wikipedia.org/wiki/Lehman%27s_laws_of_software_evolution">Lehman's fifth law of 
software evolution</a>, maintaining familiarity with any set of scripts is an ongoing challenge. Combining scripts
in this manner helps to reduce the knowledge requirements in some ways, but makes further demands in others. In
order to identify the three monsters, you must first be familiar with them. And once you have combined the three
into one, you won't benefit unless you can recognize the fourth, fifth, and so on. You have to know that a DDT
test exists and that you can take advantage of it. For this reason it is sometimes required that you review the
whole set of test scripts looking for inefficiencies and resolving discrepencies. This is common to all code
development--including test scripts.
</p>

<a name="ddt"><h3>Data Driven Testing</h3></a>
<a name="istc"><h4>Identifying Similar Test Cases</h4></a>
<p>
Testers have been trained to minimize costs and especially to avoid duplicative effort. Therefore it may
seem inconsistent that any test cases would have duplicate steps and yet they do. Consider that, at the lowest 
level, we reuse the keywords for things like finding a button, reading text, reporting data, etc. Slightly
above that level there are certain sequences of steps that are used to set up the test. For example, setting
up the simulated test data, arranging the system to automatically deal with the simulated event, and then 
kicking it all off might be common between many tests even though the details of the individual tests vary
widely.
</p>

<p>
We have found that testers, tend to look at each line of a test as completely distinct for two reasons:<br>
<ul>
  <li>The steps in different tests satisfy different requirements.</li>
  <li>The differences between those steps are the embodiment of those requirements.</li>
</ul>
</p>

<p>
Consider the example bellow. Two sentences out of a manual test procedure that verify two different aspects of
a telephone exchange system. Obviously these are different requirements, the first pertaining to returning a 
call and the second related to call forwarding. Note however, that aside from the key portions highlighted in
color, the rest of the script is identical. First isolate what is different about each test case and then one 
can remake the tests for Data Driven Testing (DDT).

<pre>
Lift the receiver, dial <span class="green">*69</span>, wait for the <span class="red">automatic reply</span>. Hang up the phone.
...
Lift the receiver, dial <span class="green">*72</span>, wait for the <span class="red">ring tone</span>, <span class="blue">dial the number</span>. Hang up the phone.
</pre>
</p>

<a name="fast"><h4>Forging a Single Test</h4></a>
<p>
Continuing with the previous example, let's build a single test script that handles both cases. To do that we
need to isolate the common portions, and <em>parameterize</em> the differences. Consider the line below which 
uses the same colors. Here we can see that there are distinct actions and there is variation in only some of
them.
</p>

<table>
  <tr><th>Start Call</th><th class="green">Number</th><th>Wait until</th><th class="red">Response</th><th class="blue">Action</th><th>End Call</th></tr>
</table>

<p>
In this version of the script we can see the differences between the two requirements as a set of distinct parameters.
In Robot Framework, these are called Arguments, but it means the same thing. They are various values that can
be put into a test at key points. Here we have a variable for the number we want to dial, the response we are
looking for, and the action (if any) that we will take. In black are the steps that are dependent on those
parameters. We can write that part of the script in black and wait for the parameters to be inserted.
</p>

<table>
  <tr><th>Start Call</th><th class="green">Number</th><th>Wait until</th><th class="red">Response</th><th class="blue">Action</th><th>End Call</th></tr>
  <tr><td>Lift the receiver, dial</td><td class="green">??</td><td>wait for the</td><td class="red">??</td><td class="blue">??</td><td>Hang up the phone</td></tr>
  <tr><td></td><td class="green">*69</td><td></td><td class="red">automatic reply</td><td class="blue">--</td><td></td></tr>
  <tr><td></td><td class="green">*72</td><td></td><td class="red">ring tone</td><td class="blue">dial the number</td><td></td></tr>
</table>

<a name="ttct"><h4>The Test Cases Table</h4></a>
<p>
In a RF test using DDT, the Test Cases are exclusively used to represent the arguments for each test case. Note here
that we have replicated the same three parameters and we have added a name for each test case on the left. The header
of each parameter (i.e. N, R, and A) are not significant in the script, but they do add some clarity. You can create
whatever name you want for the headers of this table. Note also that each test has the same number of parameters, even
if the test doesn't need them. It is easy to see that <code>--</code> is not really a parameter and the test script
can skip the action if no action is required.
</p>

<pre><code>
*** Test Cases ***          N        R                 A
Call back function          *69      automatic reply   --
Call forwarding function    *72      ring tone         dial the number
</code></pre>

<a name="aftt"><h4>Arguments for Test Template</h4></a>
<p>
In DDT, the Test Template keyword receives a set of arguments for each test case. These arguments have names that are
defined in the first line of the keyword and stay the same throughout. In this example you can see how the first test
case would run and the values that would be found in each argument. Note also that the <code>${TEST NAME}</code> variable 
is conveniently available to access the name of the test case from that table.
</p>

<pre><code>
Template Keyword
    [Arguments]    ${Number}    ${Response}       ${Action}
                   <i>#*69         automatic reply   --</i>
    Log    ${TEST NAME} <i>#Call back function</i>
</code></pre>

<a name="wttt"><h4>Writing the Test Template</h4></a>
<p>
We are going to assume quite a lot in this code sample, but the details of any test will have to be worked out completely.
Naturally, I have not written keywords for <code>Lift Phone and Dial</code>, but you can imagine that they already exist. 
Here you can see the simple code that sends the appropriate argument(s) to each keyword. This kind of script is valuable
because it allows us to add new cases with DDT.
</p>

<pre><code>
Template Keyword
    [Arguments]    ${Number}    ${Response}       ${Action}
    Log    ${TEST NAME}
    Lift Phone and Dial    ${Number}
    Wait for    ${Response}
    Run Keyword Unless    '''${Action}'''=='--'    ${Action}
    Hang Up
</code></pre>

<a name="bod"><h4>Benefits of DDT</h4></a>
<ul>
  <li>Adding a new DDT test is usually as simple as adding a single line in the Test Cases table.</li>
  <li>If there is some significantly new argument, you may need to add a new column and a couple lines of new code, but that is rare when tests are very similar.</li>
  <li>If there is any bug or defect in the script it only needs to be fixed once. Fix it once, and the improvement affects all tests in the table.</li>
  <li>If there is a new requirement (e.g. required output, data reporting, etc.) it can be put in once and it affects all tests in the table.</li>
  <li>Logically, a DDT test already is broken into several keywords so that the OQE can be defined naturally along these lines. These lines also tend to correspond to requirements.</li>
</ul>

<a name="fto"><h3>Formatting the OQE</h3></a>
<a name="romp"><h4>Rows on Main Page</h4></a>
<p>You want each test case to show up as a new row in the OQE GUI. Use the <code>New Test Event</code>
keyword to generate that row. In this image, the white arrow points to a single test created with this
keyword.</p>

<img src="rowandblock.png">
<a name="bomp"><h4>Blocks on Main Page</h4></a>
<p>On the main page each keyword shows up as a separate red/green block. While a test can be written with 
only one keyword, the OQE report with just one block is less informative. Consider that it could have
1 or 20 failures and all you would see is one red block. Instead, break up the test into multiple keywords
that make sense based on the test objectives. Major pieces of the test each turn into a separate keywords.
In the image above, the highlighted green block is the result of a single keyword.<p>

<a name="ar"><h4>Assert Row</h4></a>
<p>
Assertion rows are used to validate specific data against expectations. This might be used for any number
of requirements and the values are shown in the row. This makes the Pass/Fail determination and the original
OQE available on the same line of the report.
</p>
<img src="assertrow.png">

<a name="dr"><h4>Data Row</h4></a>
<p>
Data rows are used to record OQE which is not being validated. This could be any number of related data points
all of which show up with labels. This is OQE which may be of use in future analysis or for "information only"
purposes.
</p>
<img src="datarow.png">

<a name="pr"><h4>Picture Row</h4></a>
<p>
A picture row shows a screen shot of either a specific element OR the whole screen. These images can be used as
historical data to show that some aspect of the test did in fact occur. A common use is recording the button or
element that was clicked or manipulated. Another common use is a screen capture of the tactical picture (e.g. radar).
The image capture process is a two step process that allows the OQE library to pre-define the image name and 
location, but the image capture itself can be done with any tool that allows this sort of directed file location
(i.e. the original RF library). Note that only a thumbnail is shown in the report, but the full image is available 
when someone clicks on that image.
</p>
<img src="picturerow.png">

<a name="kt"><h4>Keyword Teardown</h4></a>
<p>
Each keyword has a built in status that becomes available during teardown. You can use <code>${KEYWORD STATUS}</code>
and <code>${KEYWORD MESSAGE}</code> to get any error that cropped up during that keyword.
in the teardown (last line of the keyword) to report on overall status. Note that in this example there is
no message because the keyword passed. The next image shows an example with the fail message.

<pre><code class="long">[TEARDOWN]&nbsp;&nbsp;&nbsp;&nbsp;Record Test Case&nbsp;&nbsp;&nbsp;&nbsp;--&nbsp;&nbsp;&nbsp;&nbsp;Menu Tests&nbsp;&nbsp;&nbsp;&nbsp;${KEYWORD STATUS}&nbsp;&nbsp;&nbsp;&nbsp;${KEYWORD MESSAGE}</code></pre>

<img src="keywordstatus.png"><br>

This example uses the <code>Record Test Case</code> to put the data into the OQE tool. Then next two arguments are:
<ul>
  <li><code>${CASE NAME}</code> - Usually reserved for the individual test case and <code>${TEST NAME}</code> is often the correct input; '--' is a null option.</li>
  <li><code>${STEP NAME}</code> - This should be the name of the step within the test and is likely the same as the name of the keyword.</li>
</ul><br>

The last two arguments are standard for the keyword teardown. The <code>${KEYWORD STATUS}</code> will be <span class="green">PASS</span> or
<span class="red">FAIL</span>. The <code>${KEYWORD MESSAGE}</code> varies quite a bit and may be used to find out what the error was.
</p>

<a name="db"><h4>Functional Groups</h4></a>
<p>When viewing the individual test report, it is an unbroken set of rows with data and checks. This makes
reading particularly hard. Instead the general unit of the report is the Functional Group. This begins with
the bright green banner indicating the name of the group, followed by a series of yellow action bars and ending
with a keyword teardown and a data break. In the middle of these groups a "soft" break can be added in a light
green to further break up the report. These can all be added to the test as keywords.</p>
<img src="databreak.png"><br>

<a name="oaa"><h3>OQE and Analysis</h3></a>
<a name="afr"><h4>Assertions for Requirements</h4></a>
<p>Assertions are the main way that test results are analyzed. In most test steps there
is something that is checked, verified, compared, or otherwise expected to be true. There
are many examples. In this case we have a specific URL that we expect to reach.  The value
of <code>${HREF}</code> and <code>${href}</code> should match. That check generates a line
in the OQE tool. Lines like this allow the tester to validate individual requirements.
<pre><code>${a1} =    Assert OQE Value    ${HTML}    REQ-5678    ${href}    ${HREF}</code></pre>

<a name="mafr"><h4>Multiple Assertions for Requirements</h4></a>
That Pass/Fail value is also recorded in the minor variables <code>${a1}</code>. Where more
than one check is related to a requirement, these can be combined. Note this example near
the end of a keyword where two variables are used to record OQE. As a tester, you can 
Pass/Fail any requirement with more complicated analysis in this way.</p>

<pre><code>
   IF    '''${a1}'''=='''${a2}''' and '''${a2}'''=='''PASS'''
      Record Test Case    --    Found Matching Menu Item    ${a1}    REQ-1234
   ELSE
      Record Test Case    --    Found Matching Menu Item    FAIL    REQ-1234
   END
</code></pre>
  </body>
</html>
